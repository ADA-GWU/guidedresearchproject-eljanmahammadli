{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c6f901",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "During the past week, two significant advancements have been made in my summer research project. The first one is related to the development of next predictive models - specifically, I have successfully implemented a Recurrent Neural Network (RNN) and its more sophisticated variant, Gated Recurrent Unit (GRU). The second big improvement this week was about bringing together different parts of our system into one unfied framework.\n",
    "\n",
    "In regards to the first advancement, the implementation of the RNN and GRU models has been a crucial step forward. These models represent an evolution to process sequence data, offering enhanced predictive capabilities compared to previous models. Building these models required a deep understanding of their underlying mechanisms, and their successful implementation marks a significant milestone in the project.\n",
    "\n",
    "The second noteworthy accomplishment this week is related to the system architecture. After independently developing various models such as Bigram, MLP, WaveNet, RNN (and Transformer, which is currently in progress), I have started to integrate all these components into a single Python script along with two helper scripts. This consolidated framework will provide a command-line interface that allows users to effortlessly train and inference models on any given word collection. By selecting their preferred model and parameters, users can easily customize the system according to their specific needs.\n",
    "\n",
    "This week's work has not only advanced capabilities in terms of predictive modeling but has also significantly improved the user accessibility and efficiency of the proposed system. Looking forward, I aim to complete the Transformer model and fully integrate it into the unified framework, thereby offering an even broader range of predictive models for users to choose from.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfeb11",
   "metadata": {},
   "source": [
    "# Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f2c8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# modelling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# dataset reading and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# helper functions for both dataset preparation and model training and inference\n",
    "from dataset_utils import clean_and_train_test_split, CharacterDataset, ContinuousDataLoader\n",
    "from model_helpers import ModelConfig, display_samples, create_tokens, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceaf088",
   "metadata": {},
   "source": [
    "## Spelling-out `model_helpers` and `dataset_utils`\n",
    "<br>\n",
    "\n",
    "### `model_helpers`\n",
    "\n",
    "This set of functions and classes mainly aids in managing and manipulating the models and their outputs. The functions in this module include those for generating new tokens, displaying model-generated samples, and evaluating a model's performance.\n",
    "\n",
    "1. `ModelConfig` class: This is a configuration class for the model parameters. It's used to store hyperparameters like the number of layers in the model (`n_layer`), the embedding size (`n_embd` and `n_embd2`), and the number of heads in the model (`n_head`), along with properties of the data such as the block size (`block_size`) and vocabulary size (`vocab_size`).\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cdab3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    This is a simple data class for storing model configuration settings. It includes settings related to the model architecture, such as the number of layers, the embedding size, and the number of heads, as well as settings related to the input data, such as the block size and vocabulary size.\n",
    "    \"\"\"\n",
    "    block_size: int = None # input sequences length\n",
    "    vocab_size: int = None # (0, vocab_size -1)\n",
    "    # model parameters for different layers\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c99c90",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "2. `create_tokens` function: This function generates new tokens or characters from the given model. It starts from a provided sequence of indices, and based on the predictions of the model, it generates new tokens up to a maximum length defined by `max_token_creation`. If `sampling` is `True`, it will sample the next token based on the model's output distribution. Otherwise, it picks the token with the highest probability. If `top_k` is provided, it trims the predictions to only consider the top-k most probable tokens. This function returns a new sequence of tokens.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36a6b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def create_tokens(model, sequence_indices, max_token_creation, sampling=False, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate new tokens from the given model, starting from a provided sequence of indices. This function can either sample the next token based on the model's output distribution or pick the token with the highest probability. It can also limit the prediction to the top-k most probable tokens.\n",
    "    \"\"\"\n",
    "    sequence_limit = model.get_block_size()\n",
    "    for _ in range(max_token_creation):\n",
    "        # If the sequence context grows too large, it must be trimmed to sequence_limit\n",
    "        sequence_condition = sequence_indices if sequence_indices.size(1) <= sequence_limit else sequence_indices[:, -sequence_limit:]\n",
    "        # Pass the model forward to get the logits for the index in the sequence\n",
    "        logits, _ = model(sequence_condition)\n",
    "        logits = logits[:, -1, :]\n",
    "        # Optionally trim the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # Apply softmax to convert logits to (normalized) probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        # Either sample from the distribution or take the most likely element\n",
    "        if sampling:\n",
    "            next_index = torch.multinomial(probabilities, num_samples=1)\n",
    "        else:\n",
    "            _, next_index = torch.topk(probabilities, k=1, dim=-1)\n",
    "        # Append sampled index to the ongoing sequence and continue\n",
    "        sequence_indices = torch.cat((sequence_indices, next_index), dim=1)\n",
    "\n",
    "    return sequence_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75cc95",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "3. `display_samples` function: This function displays some generated samples from the model. It first creates an initial sequence of zeros and generates subsequent tokens using the `create_tokens` function. Then it checks if the generated samples are in the training set, testing set, or if they are completely new words. It finally prints out the generated samples.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e651306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples(device, train_dataset, model, quantity=10):\n",
    "    \"\"\"\n",
    "    Display some generated samples from the model. This function generates samples, checks if they are in the training set, testing set, or completely new, and prints out the generated samples.\n",
    "    \"\"\"    \n",
    "    starting_input = torch.zeros(quantity, 1, dtype=torch.long).to(device)\n",
    "    generation_steps = train_dataset.get_output_length() - 1 # -1 due to initial <START> token (index 0)\n",
    "    sampled_input = create_tokens(model, starting_input, generation_steps, top_k=None, sampling=True).to(device)\n",
    "    training_words, testing_words, novel_words = [], [], []\n",
    "    for i in range(sampled_input.size(0)):\n",
    "        # Obtain the i'th row of sampled integers, as python list\n",
    "        sequence_row = sampled_input[i, 1:].tolist() # Remove the <START> token\n",
    "        # Token 0 is the <STOP> token, thus we truncate the output sequence at that point\n",
    "        stop_index = sequence_row.index(0) if 0 in sequence_row else len(sequence_row)\n",
    "        sequence_row = sequence_row[:stop_index]\n",
    "        sample_word = train_dataset.decode(sequence_row)\n",
    "        # Check which words are in the training/testing set and which are new\n",
    "        if train_dataset.contains(sample_word):\n",
    "            training_words.append(sample_word)\n",
    "        elif train_dataset.contains(sample_word):\n",
    "            testing_words.append(sample_word)\n",
    "        else:\n",
    "            novel_words.append(sample_word)\n",
    "    print('-'*50)\n",
    "    for word_list, descriptor in [(training_words, 'in training'), (testing_words, 'in testing'), (novel_words, 'new')]:\n",
    "        print(f\"{len(word_list)} samples that are {descriptor}:\")\n",
    "        for word in word_list:\n",
    "            print(word)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d54c35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(model, dataset, device, batch_size=50, max_batches=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataset. This function calculates the average loss of the model on the dataset, optionally limiting the evaluation to a certain number of batches.\n",
    "    \"\"\"\n",
    "    model.eval() # evaluation mode\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        X, Y = batch\n",
    "        logits, loss = model(X, Y)\n",
    "        losses.append(loss.item())\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "    mean_loss = torch.tensor(losses).mean().item()\n",
    "    model.train()\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8badfd0",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "4. `evaluate` function: This function evaluates the model on a provided dataset. It creates a DataLoader for the dataset, runs the model in evaluation mode, and computes the average loss on the dataset. The model is then set back to training mode. If `max_batches` is specified, it limits the number of batches to evaluate. The function returns the average loss.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69bf1d6",
   "metadata": {},
   "source": [
    "### `dataset_utils`\n",
    "\n",
    "This set of functions and classes is primarily responsible for handling the dataset for training and evaluating the models. The functions in this module include those for data cleaning, splitting the data, encoding and decoding data samples, and providing continuous data loading for model training.\n",
    "<br><br>\n",
    "\n",
    "1. `clean_and_train_test_split`: This function performs several data preparation tasks. It reads a CSV file containing company names, cleans the names by removing leading/trailing spaces and empty names, and splits the data into a training set and a test set. It also creates an alphabet from the unique characters in the company names and prints out some information about the dataset. The function finally returns `CharacterDataset` objects for the training and test sets.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fee13ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_train_test_split():\n",
    "    \"\"\"\n",
    "    Reads a CSV file of company names, cleans the names, creates an alphabet from the unique characters, splits the data into a training set and a test set, and returns `CharacterDataset` objects for the training and test sets.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "      \"../data/cleansed_layer/companies_usa_size_over_10.csv\", usecols=[\"name\"]\n",
    "    )\n",
    "    \n",
    "    # calling \"words\" instead of names as input data can be any collection of words\n",
    "    words = df.name.to_list()\n",
    "\n",
    "    # cleaning the data, removing and leading or ending spaces and deleting empty words\n",
    "    words = [w.strip() for w in words] \n",
    "    words = [w for w in words if w]\n",
    "    alphabet = sorted(list(set(''.join(words)))) # constructing the alphabets\n",
    "    max_length = max(len(w) for w in words)\n",
    "    print(f\"word size in the data: {len(words)}\")\n",
    "    print(f\"word with the maximum length: {max_length}\")\n",
    "    print(f\"number of characters in the alphabet: {len(alphabet)}\")\n",
    "    print(\"alphabet: \", ''.join(alphabet))\n",
    "\n",
    "    # train/test split (we'll use the test set to evaluate the model)\n",
    "    test_set_size = min(1000, int(len(words) * 0.1))\n",
    "    randp = torch.randperm(len(words)).tolist()\n",
    "    train = [words[i] for i in randp[:-test_set_size]]\n",
    "    test = [words[i] for i in randp[-test_set_size:]]\n",
    "    print(f\"train set size: {len(train)}, test set size: {len(test)}\")\n",
    "\n",
    "    train_dataset = CharacterDataset(train, alphabet, max_length)\n",
    "    test_dataset = CharacterDataset(test, alphabet, max_length)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c52fe",
   "metadata": {},
   "source": [
    "2. `CharacterDataset` class: This class extends PyTorch's `Dataset` class and is used for handling the company names dataset. It's initialized with a list of words (company names), an alphabet, and the maximum word length. It provides methods for:\n",
    "   - Getting the size of the dataset\n",
    "   - Checking if a word is in the dataset\n",
    "   - Getting the size of the vocabulary\n",
    "   - Getting the maximum sequence length\n",
    "   - Encoding a word to indices and decoding indices to a word\n",
    "   - Getting an item from the dataset by index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1142798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A `Dataset` subclass for handling the company names dataset. Provides methods for encoding and decoding words, checking if a word is in the dataset, getting the size of the dataset, getting the size of the vocabulary, getting the maximum sequence length, and getting an item from the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, words, alphabet, max_word_length):\n",
    "        self.words = words\n",
    "        self.alphabet = alphabet\n",
    "        self.max_word_length = max_word_length\n",
    "        self.stoi = {ch:i+1 for i,ch in enumerate(alphabet)} # string to index encoding\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} # index to string decoding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.alphabet) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def get_output_length(self):\n",
    "        return self.max_word_length + 1 # the longest word + 1 for the SOS token\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        x = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        y = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        x[1:1+len(ix)] = ix\n",
    "        y[:len(ix)] = ix\n",
    "        y[len(ix)+1:] = -1\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb72bcb",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34879a5f",
   "metadata": {},
   "source": [
    "3. `ContinuousDataLoader` class: This class creates an infinite data loader for a given dataset. The loader repeatedly iterates over the dataset in a random order. It provides a `get_next` method to get the next batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5262af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousDataLoader:\n",
    "    \"\"\"\n",
    "    A class for creating an infinite data loader for a given dataset. The loader repeatedly iterates over the dataset in a random order. Provides a `get_next` method to get the next batch of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, **loader_args):\n",
    "        infinite_sampler = torch.utils.data.RandomSampler(data_source, replacement=True, num_samples=int(1e10))\n",
    "        self.infinite_loader = DataLoader(data_source, sampler=infinite_sampler, **loader_args)\n",
    "        self.data_iterator = iter(self.infinite_loader)\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            data_batch = next(self.data_iterator)\n",
    "        except StopIteration:\n",
    "            self.data_iterator = iter(self.infinite_loader)\n",
    "            data_batch = next(self.data_iterator)\n",
    "        return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75183008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducing results\n",
    "seed = 10110609\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1264f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "work_dir = 'out' # model export directory\n",
    "top_k = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14913ab5",
   "metadata": {},
   "source": [
    "# Constructing RNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e076e8",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, or the spoken word. RNNs are called \"recurrent\" because they perform the same task for every element of a sequence, with the output depending on the previous computations. This recurrence mechanism allows information to be passed from one step of the sequence to the next.\n",
    "\n",
    "In the context of character-based language modeling, RNNs are used to predict the next character in a sequence given the sequence of previous characters. Each input to the RNN corresponds to a character. The RNN maintains an internal state that it uses to capture the information about the characters it has seen so far. The output at each step is a probability distribution over the next character.\n",
    "\n",
    "<b>Note: For the forward pass of the architecture please refer to the Docstrings for below codes.<b>\n",
    "\n",
    "Also, Figure 1 very intuitive for understanding the forward pass of the vanilla RNN Cell.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f38a42",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77df2d",
   "metadata": {},
   "source": [
    "![images/rnn_cell.png](images/rnn_cell.png)\n",
    "<b><center> Figure 1: RNN Cell (Image sourced from the DeepLearning.AI Sequence Models course.) </center><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffe55005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic RNN cell.\n",
    "\n",
    "    This class represents the basic building block of a Recurrent Neural Network (RNN). \n",
    "    An RNN cell takes the current input and the previous hidden state to produce the \n",
    "    new hidden state. This operation is performed for every element in the input sequence.\n",
    "\n",
    "    Args:\n",
    "        config (ModelConfig): The configuration object containing the model parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the RNN cell with a linear layer.\n",
    "\n",
    "        The linear layer transforms the concatenated input and hidden state to the \n",
    "        new hidden state.\n",
    "\n",
    "        Args:\n",
    "            config (ModelConfig): The configuration object containing the model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.xh_to_h = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2) # (128, 64)\n",
    "\n",
    "    def forward(self, xt, hprev):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the RNN cell.\n",
    "\n",
    "        The forward pass involves concatenating the input and the previous hidden state, \n",
    "        and passing it through the linear layer. The output of the linear layer is \n",
    "        passed through a tanh activation function to produce the new hidden state.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): The input tensor at the current timestep.\n",
    "            hprev (torch.Tensor): The hidden state at the previous timestep.\n",
    "\n",
    "        Returns:\n",
    "            ht (torch.Tensor): The hidden state at the current timestep.\n",
    "        \"\"\"\n",
    "        # xt: input tensor\n",
    "        # hprev: previous hidden state\n",
    "        xh = torch.cat([xt, hprev], dim=1) # concat along y-axis\n",
    "        ht = F.tanh(self.xh_to_h(xh)) # obtain new hidden state\n",
    "        return ht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b124b",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443cb0e6",
   "metadata": {},
   "source": [
    "While vanilla Recurrent Neural Networks (RNNs) are powerful models for sequence data, they suffer from the \"vanishing gradients\" problem. This problem occurs when gradients are backpropagated through time, from the output end of the sequence to the input end. As the sequence gets longer, these gradients can become increasingly small, until they practically vanish. This makes the model forget the earlier inputs. This issue poses a significant problem in tasks such as language modeling where the model needs to remember longer sequences of characters to make accurate predictions.\n",
    "\n",
    "Gated Recurrent Units (GRUs) were introduced to solve this issue. GRUs incorporate gating mechanisms that allow each recurrent unit to adaptively control the flow of information from one time step to the next. This makes them better suited for handling longer sequences and dependencies between characters that are far apart.\n",
    "\n",
    "In the context of character-based language modeling, a GRU can decide to remember a character that happened long ago if it believes it would be helpful in predicting future characters.\n",
    "\n",
    "<b>Note: For the forward pass of the architecture please refer to the Docstrings for below codes.<b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57dbdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Recurrent Unit (GRU) cell.\n",
    "\n",
    "    The GRU cell is an improved version of the vanilla RNN cell that incorporates gating \n",
    "    mechanisms, specifically update and reset gates. These gates allow the GRU cell to \n",
    "    adaptively control the flow of information from one time step to the next, thereby \n",
    "    mitigating the vanishing gradient problem.\n",
    "\n",
    "    Args:\n",
    "        config (ModelConfig): The configuration object containing the model parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "    \"\"\"\n",
    "    Gated Recurrent Unit (GRU) cell.\n",
    "\n",
    "    The GRU cell is an improved version of the vanilla RNN cell that incorporates gating \n",
    "    mechanisms, specifically update and reset gates. These gates allow the GRU cell to \n",
    "    adaptively control the flow of information from one time step to the next, thereby \n",
    "    mitigating the vanishing gradient problem.\n",
    "\n",
    "    Args:\n",
    "        config (ModelConfig): The configuration object containing the model parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the GRU cell with three linear layers.\n",
    "\n",
    "        The linear layers are used to compute the values of the update gate, reset gate, \n",
    "        and candidate hidden state.\n",
    "\n",
    "        Args:\n",
    "            config (ModelConfig): The configuration object containing the model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # input, forget, output, gate\n",
    "        self.xh_to_z = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.xh_to_r = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.xh_to_hbar = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "\n",
    "    def forward(self, xt, hprev):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the GRU cell.\n",
    "\n",
    "        The forward pass involves computing the update and reset gates, calculating the \n",
    "        candidate hidden state based on the reset gate, and then blending the previous \n",
    "        hidden state and the candidate hidden state using the update gate to produce \n",
    "        the new hidden state.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): The input tensor at the current timestep.\n",
    "            hprev (torch.Tensor): The hidden state at the previous timestep.\n",
    "\n",
    "        Returns:\n",
    "            ht (torch.Tensor): The hidden state at the current timestep.\n",
    "        \"\"\"\n",
    "        xh = torch.cat([xt, hprev], dim=1)\n",
    "        r = F.sigmoid(self.xh_to_r(xh)) # reset gate\n",
    "        hprev_reset = r * hprev\n",
    "        xhr = torch.cat([xt, hprev_reset], dim=1)\n",
    "        hbar = F.tanh(self.xh_to_hbar(xhr)) # candidate hidden state\n",
    "        z = F.sigmoid(self.xh_to_z(xh)) # update gate\n",
    "        ht = (1 - z) * hprev + z * hbar # new hidden state\n",
    "        return ht"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
