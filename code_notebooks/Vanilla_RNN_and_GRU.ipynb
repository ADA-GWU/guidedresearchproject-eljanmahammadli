{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c6f901",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "During the past week, two significant advancements have been made in my summer research project. The first one is related to the development of next predictive models - specifically, I have successfully implemented a Recurrent Neural Network (RNN) and its more sophisticated variant, Gated Recurrent Unit (GRU). The second big improvement this week was about bringing together different parts of our system into one unfied framework.\n",
    "\n",
    "In regards to the first advancement, the implementation of the RNN and GRU models has been a crucial step forward. These models represent an evolution to process sequence data, offering enhanced predictive capabilities compared to previous models. Building these models required a deep understanding of their underlying mechanisms, and their successful implementation marks a significant milestone in the project.\n",
    "\n",
    "The second noteworthy accomplishment this week is related to the system architecture. After independently developing various models such as Bigram, MLP, WaveNet, RNN (and Transformer, which is currently in progress), I have started to integrate all these components into a single Python script along with two helper scripts. This consolidated framework will provide a command-line interface that allows users to effortlessly train and inference models on any given word collection. By selecting their preferred model and parameters, users can easily customize the system according to their specific needs.\n",
    "\n",
    "This week's work has not only advanced capabilities in terms of predictive modeling but has also significantly improved the user accessibility and efficiency of the proposed system. Looking forward, I aim to complete the Transformer model and fully integrate it into the unified framework, thereby offering an even broader range of predictive models for users to choose from.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfeb11",
   "metadata": {},
   "source": [
    "# Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f2c8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# modelling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# dataset reading and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# helper functions for both dataset preparation and model training and inference\n",
    "from dataset_utils import clean_and_train_test_split, CharacterDataset, ContinuousDataLoader\n",
    "from model_helpers import ModelConfig, display_samples, create_tokens, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceaf088",
   "metadata": {},
   "source": [
    "## Spelling-out `model_helpers` and `dataset_utils`\n",
    "<br>\n",
    "\n",
    "### `model_helpers`\n",
    "\n",
    "This set of functions and classes mainly aids in managing and manipulating the models and their outputs. The functions in this module include those for generating new tokens, displaying model-generated samples, and evaluating a model's performance.\n",
    "\n",
    "1. `ModelConfig` class: This is a configuration class for the model parameters. It's used to store hyperparameters like the number of layers in the model (`n_layer`), the embedding size (`n_embd` and `n_embd2`), and the number of heads in the model (`n_head`), along with properties of the data such as the block size (`block_size`) and vocabulary size (`vocab_size`).\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cdab3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    This is a simple data class for storing model configuration settings. It includes settings related to the model architecture, such as the number of layers, the embedding size, and the number of heads, as well as settings related to the input data, such as the block size and vocabulary size.\n",
    "    \"\"\"\n",
    "    block_size: int = None # input sequences length\n",
    "    vocab_size: int = None # (0, vocab_size -1)\n",
    "    # model parameters for different layers\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c99c90",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "2. `create_tokens` function: This function generates new tokens or characters from the given model. It starts from a provided sequence of indices, and based on the predictions of the model, it generates new tokens up to a maximum length defined by `max_token_creation`. If `sampling` is `True`, it will sample the next token based on the model's output distribution. Otherwise, it picks the token with the highest probability. If `top_k` is provided, it trims the predictions to only consider the top-k most probable tokens. This function returns a new sequence of tokens.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36a6b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def create_tokens(model, sequence_indices, max_token_creation, sampling=False, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate new tokens from the given model, starting from a provided sequence of indices. This function can either sample the next token based on the model's output distribution or pick the token with the highest probability. It can also limit the prediction to the top-k most probable tokens.\n",
    "    \"\"\"\n",
    "    sequence_limit = model.get_block_size()\n",
    "    for _ in range(max_token_creation):\n",
    "        # If the sequence context grows too large, it must be trimmed to sequence_limit\n",
    "        sequence_condition = sequence_indices if sequence_indices.size(1) <= sequence_limit else sequence_indices[:, -sequence_limit:]\n",
    "        # Pass the model forward to get the logits for the index in the sequence\n",
    "        logits, _ = model(sequence_condition)\n",
    "        logits = logits[:, -1, :]\n",
    "        # Optionally trim the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # Apply softmax to convert logits to (normalized) probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        # Either sample from the distribution or take the most likely element\n",
    "        if sampling:\n",
    "            next_index = torch.multinomial(probabilities, num_samples=1)\n",
    "        else:\n",
    "            _, next_index = torch.topk(probabilities, k=1, dim=-1)\n",
    "        # Append sampled index to the ongoing sequence and continue\n",
    "        sequence_indices = torch.cat((sequence_indices, next_index), dim=1)\n",
    "\n",
    "    return sequence_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75cc95",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "3. `display_samples` function: This function displays some generated samples from the model. It first creates an initial sequence of zeros and generates subsequent tokens using the `create_tokens` function. Then it checks if the generated samples are in the training set, testing set, or if they are completely new words. It finally prints out the generated samples.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e651306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples(device, train_dataset, model, quantity=10):\n",
    "    \"\"\"\n",
    "    Display some generated samples from the model. This function generates samples, checks if they are in the training set, testing set, or completely new, and prints out the generated samples.\n",
    "    \"\"\"    \n",
    "    starting_input = torch.zeros(quantity, 1, dtype=torch.long).to(device)\n",
    "    generation_steps = train_dataset.get_output_length() - 1 # -1 due to initial <START> token (index 0)\n",
    "    sampled_input = create_tokens(model, starting_input, generation_steps, top_k=None, sampling=True).to(device)\n",
    "    training_words, testing_words, novel_words = [], [], []\n",
    "    for i in range(sampled_input.size(0)):\n",
    "        # Obtain the i'th row of sampled integers, as python list\n",
    "        sequence_row = sampled_input[i, 1:].tolist() # Remove the <START> token\n",
    "        # Token 0 is the <STOP> token, thus we truncate the output sequence at that point\n",
    "        stop_index = sequence_row.index(0) if 0 in sequence_row else len(sequence_row)\n",
    "        sequence_row = sequence_row[:stop_index]\n",
    "        sample_word = train_dataset.decode(sequence_row)\n",
    "        # Check which words are in the training/testing set and which are new\n",
    "        if train_dataset.contains(sample_word):\n",
    "            training_words.append(sample_word)\n",
    "        elif train_dataset.contains(sample_word):\n",
    "            testing_words.append(sample_word)\n",
    "        else:\n",
    "            novel_words.append(sample_word)\n",
    "    print('-'*50)\n",
    "    for word_list, descriptor in [(training_words, 'in training'), (testing_words, 'in testing'), (novel_words, 'new')]:\n",
    "        print(f\"{len(word_list)} samples that are {descriptor}:\")\n",
    "        for word in word_list:\n",
    "            print(word)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d54c35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(model, dataset, device, batch_size=50, max_batches=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataset. This function calculates the average loss of the model on the dataset, optionally limiting the evaluation to a certain number of batches.\n",
    "    \"\"\"\n",
    "    model.eval() # evaluation mode\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        X, Y = batch\n",
    "        logits, loss = model(X, Y)\n",
    "        losses.append(loss.item())\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "    mean_loss = torch.tensor(losses).mean().item()\n",
    "    model.train()\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8badfd0",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "4. `evaluate` function: This function evaluates the model on a provided dataset. It creates a DataLoader for the dataset, runs the model in evaluation mode, and computes the average loss on the dataset. The model is then set back to training mode. If `max_batches` is specified, it limits the number of batches to evaluate. The function returns the average loss.\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
